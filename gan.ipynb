{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from utils.custom_layers import InstanceNormalization, LayerNormalization\n",
    "from utils.data import tfr_dataset_eager, parse_img_label_tfr\n",
    "from utils.models import model_up_to, add_norm\n",
    "from utils.viz import random_sample_grid, interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "#svhn = False  # set True to train on svhn, else MNIST\n",
    "cifer = True   #  set True to train on cifar10, else MNIST\n",
    "batch_size = 128  # this is a \"half batch\"!\n",
    "train_steps = 25000 if cifer else 10000\n",
    "dim_noise = 100  # tune\n",
    "label_smoothing = 0.9  # tune\n",
    "\n",
    "if cifer:\n",
    "    train_files = [\"data/tfrs/cifar10_train.tfr\"]\n",
    "    test_files = [\"data/tfrs/cifar10_test.tfr\"]\n",
    "    parse_fn = lambda x: parse_img_label_tfr(x, (32, 32, 3))\n",
    "else:\n",
    "    train_files = [\"/cache/tfrs/mnist_train.tfr\"]\n",
    "    test_files = [\"/cache/tfrs/mnist_test.tfr\"]\n",
    "    parse_fn = lambda x: parse_img_label_tfr(x, (32, 32, 1))\n",
    "\n",
    "data = tfr_dataset_eager(train_files, batch_size, parse_fn, shufrep=600000 if cifer else 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature matching loss for generator (from improved techniques for training GANs)\n",
    "def feature_match(real, gened):\n",
    "    real_feats = disc_part(real)\n",
    "    gen_feats = disc_part(gened)\n",
    "    return tf.norm(tf.reduce_mean(real_feats, axis=0) - tf.reduce_mean(gen_feats, axis=0))\n",
    "\n",
    "# FC discriminator; can comment out the MBD layer or uncomment the MBSD layer if desired\n",
    "def enc_fc_mnist(final_dim, use_bn=True, h_act=layers.LeakyReLU, clip=None,\n",
    "                 norm=layers.BatchNormalization):\n",
    "    const = (lambda v: tf.clip_by_value(v, -clip, clip)) if clip else None\n",
    "    seq = [layers.Flatten(),\n",
    "           layers.Dense(512, kernel_constraint=const, bias_constraint=const,\n",
    "                        use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.Dense(256, kernel_constraint=const, bias_constraint=const,\n",
    "                        use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.Dense(128, kernel_constraint=const, bias_constraint=const,\n",
    "                        use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           #MBD(16, 20),\n",
    "           #MBSD(False),\n",
    "           layers.Dense(final_dim, kernel_constraint=const,\n",
    "                        bias_constraint=const)]\n",
    "    if use_bn:\n",
    "        seq = add_norm(seq, norm=norm)\n",
    "    return tf.keras.Sequential(seq)\n",
    "\n",
    "# convolutional discriminator; once again, can use MBD or MBSD or neither\n",
    "def enc_conv_mnist(final_dim, use_bn=True, h_act=layers.LeakyReLU, clip=None,\n",
    "                   norm=layers.BatchNormalization):\n",
    "    const = (lambda v: tf.clip_by_value(v, -clip, clip)) if clip else None\n",
    "    seq = [layers.Conv2D(32, 4, padding=\"same\", kernel_constraint=const,\n",
    "                         bias_constraint=const, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.AveragePooling2D(padding=\"same\"),\n",
    "           layers.Conv2D(64, 4, padding=\"same\", kernel_constraint=const,\n",
    "                         bias_constraint=const, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.AveragePooling2D(padding=\"same\"),\n",
    "           layers.Conv2D(128, 4, padding=\"same\", kernel_constraint=const,\n",
    "                         bias_constraint=const, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.AveragePooling2D(padding=\"same\"),\n",
    "           layers.Conv2D(256, 4, padding=\"same\", kernel_constraint=const,\n",
    "                         bias_constraint=const, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.AveragePooling2D(padding=\"same\"),\n",
    "           #MBSD(True),\n",
    "           layers.Flatten(),\n",
    "           #MBD(16, 20),\n",
    "           layers.Dense(final_dim, kernel_constraint=const,\n",
    "                        bias_constraint=const, use_bias=not use_bn)]\n",
    "    if use_bn:\n",
    "        seq = add_norm(seq, norm=norm)\n",
    "    return tf.keras.Sequential(seq)\n",
    "\n",
    "def gen_fc_mnist(use_bn=True, h_act=layers.LeakyReLU, final_act=tf.nn.sigmoid,\n",
    "                 norm=layers.BatchNormalization, channels=1):\n",
    "    seq = [layers.Dense(256, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.Dense(512, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.Dense(1024, use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.Dense(32*32*channels, final_act),\n",
    "           layers.Reshape((32, 32, channels))]\n",
    "    if use_bn:\n",
    "        seq = add_norm(seq, up_to=-2, norm=norm)\n",
    "    return tf.keras.Sequential(seq)\n",
    "\n",
    "def gen_conv_mnist_nn(use_bn=True, h_act=layers.LeakyReLU,\n",
    "                      final_act=tf.nn.sigmoid, norm=layers.BatchNormalization,\n",
    "                      channels=1):\n",
    "    seq = [layers.Lambda(lambda x: x[:, tf.newaxis, tf.newaxis, :]),\n",
    "           layers.UpSampling2D(),\n",
    "           layers.Conv2D(256, 4, padding=\"same\", use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.UpSampling2D(),\n",
    "           layers.Conv2D(128, 4, padding=\"same\", use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.UpSampling2D(),\n",
    "           layers.Conv2D(64, 4, padding=\"same\", use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.UpSampling2D(),\n",
    "           layers.Conv2D(32, 4, padding=\"same\", use_bias=not use_bn),\n",
    "           h_act(),\n",
    "           layers.UpSampling2D(),\n",
    "           layers.Conv2D(channels, 4, padding=\"same\", activation=final_act)]\n",
    "    if use_bn:\n",
    "        seq = add_norm(seq, norm=norm)\n",
    "    return tf.keras.Sequential(seq)\n",
    "\n",
    "def noise_fn(n_samples): \n",
    "    return tf.random.uniform([n_samples, dim_noise], minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = True\n",
    "noise_scale = 2. * 1/256  # add noise to discriminator inputs\n",
    "\n",
    "if conv:\n",
    "    discriminator = enc_conv_mnist(1, use_bn=True, norm=InstanceNormalization)\n",
    "    generator = gen_conv_mnist_nn(use_bn=True, channels=3 if cifer else 1, norm=InstanceNormalization)\n",
    "else:\n",
    "    discriminator = enc_fc_mnist(1, use_bn=True, norm=layers.LayerNormalization)\n",
    "    generator = gen_fc_mnist(use_bn=True, channels=3 if cifer else 1, norm=layers.LayerNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for feature matching -- hardcoded -- bad.\n",
    "# basically builds a model that incudes the first N layers.\n",
    "# right now n=16.\n",
    "# but this will be different for convolutional and MLP discriminator...\n",
    "# note that layer index includes conv, pooling, activations.\n",
    "# easiest way is to check model.layers and decide up to which index one would like\n",
    "# to run it.\n",
    "disc_part = model_up_to(discriminator, 16)\n",
    "\n",
    "\n",
    "loss = tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "lr = tf.optimizers.schedules.PolynomialDecay(0.0003, train_steps, 0.00001)\n",
    "\n",
    "gen_opt = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "disc_opt = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train GAN\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def train(batch):\n",
    "    batch_dim = tf.shape(batch)[0]\n",
    "    \n",
    "    # prepare mixed batch for discriminator training\n",
    "    # For batchnorm to work better, we feed only real images, then only \n",
    "    # generated ones and then average the loss\n",
    "    gen_batch = generator(noise_fn(batch_dim))\n",
    "    real_labels = label_smoothing*tf.ones([batch_dim, 1])\n",
    "    gen_labels = tf.zeros([batch_dim, 1])\n",
    "    with tf.GradientTape() as d_tape:\n",
    "        d_loss_real = loss(real_labels, discriminator(batch + tf.random.normal(tf.shape(batch), stddev=noise_scale)))\n",
    "        d_loss_fake = loss(gen_labels, discriminator(gen_batch + tf.random.normal(tf.shape(gen_batch), stddev=noise_scale)))\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "    \n",
    "    # fresh generated batch for generator training\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g_tape:\n",
    "        for vari in generator.trainable_variables:\n",
    "            g_tape.watch(vari)\n",
    "        gen_only_batch = generator(noise_fn(2*batch_dim))\n",
    "        g_loss = loss(label_smoothing*tf.ones([2*batch_dim, 1]),\n",
    "                      discriminator(gen_only_batch + tf.random.normal(tf.shape(gen_only_batch), stddev=noise_scale)))\n",
    "        #g_loss = feature_match(batch, gen_only_batch)\n",
    "    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss\n",
    "    \n",
    "\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "for step, (img_batch, _) in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    g_loss, d_loss = train(img_batch)\n",
    "    \n",
    "    if not step % 250:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Gen Loss\", g_loss)\n",
    "        print(\"Disc Loss\", d_loss)\n",
    "        tf.keras.backend.set_learning_phase(0)\n",
    "        grid = random_sample_grid(generator, noise_fn)\n",
    "        tf.keras.backend.set_learning_phase(1)\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate samples\"\"\"\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "# use the \"truncation trick\" for sampling\n",
    "# supposedly increases sample quality\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tdist = tfd.TruncatedNormal(0., 1., -0.5, 0.5)\n",
    "\n",
    "t_noise_fn = lambda x: tdist.sample([x, dim_noise])\n",
    "grid = random_sample_grid(generator, t_noise_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interpolation behavior\"\"\"\n",
    "codes = noise_fn(2)\n",
    "a_code = codes[0]\n",
    "b_code = codes[1]\n",
    "interpolate(a_code, b_code, gen=generator, method=\"slerp\", figsize=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
